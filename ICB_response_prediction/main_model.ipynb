{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this code employ a hybrid feature selection approach combined with machine learning to predict melanoma patients' responses to ICB therapy. The feature selection process integrate two filter methods, univariate ROC-AUC and RliefF, along with one wrapper method, SVM-RFE. then ensemble voting classifier used for evaluate the model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split, LeaveOneOut, KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, classification_report\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "import xgboost as xgb\n",
    "from feature_engine.selection import DropDuplicateFeatures, DropConstantFeatures\n",
    "from sklearn.feature_selection import SelectFromModel, VarianceThreshold\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN, BorderlineSMOTE, SVMSMOTE\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import roc_auc_score, plot_roc_curve\n",
    "from yellowbrick.classifier import ROCAUC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input the data (gene expression array) and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['riaz', 'gide', 'van allen', 'hugo', 'lee']\n",
    "name= filenames[1]\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['riaz', 'gide', 'van allen', 'hugo', 'lee']\n",
    "name= filenames[1]\n",
    "\n",
    "# Read logged data\n",
    "logged_data = pd.read_csv('data/' + name + '_logged_data.csv', index_col='Unnamed: 0').T\n",
    "print(logged_data.shape)\n",
    "print(logged_data.head())\n",
    "\n",
    "# Read labels\n",
    "label = pd.read_csv('data/' + name + '.Pre.Samples.corr.csv', index_col='Unnamed: 0')\n",
    "print(label.shape)\n",
    "print(label.head())\n",
    "\n",
    "# Count of labels\n",
    "print(label['Resp_NoResp'].value_counts())\n",
    "\n",
    "# Normalized count of labels\n",
    "print(label['Resp_NoResp'].value_counts(normalize=True))\n",
    "\n",
    "# Label encoding\n",
    "y = label.replace({'No_Response': 0, 'Response': 1})\n",
    "labels = ['No_Response', 'Response']  # for plotting convenience later on\n",
    "\n",
    "print(y.head())\n",
    "print(y['Resp_NoResp'].head())\n",
    "print(y.iloc[:, 0].head())\n",
    "print(y.iloc[:, 0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the immune related genes from immport\n",
    "IRG= pd.read_csv('IRG.csv')\n",
    "print(IRG.shape)\n",
    "IRG.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (Y)\n",
    "X = logged_data\n",
    "Y = y.iloc[:, 0]\n",
    "\n",
    "# Oversampling using BorderlineSMOTE\n",
    "ada = BorderlineSMOTE(\n",
    "    sampling_strategy='auto',  # samples only the minority class\n",
    "    random_state=0,  # for reproducibility\n",
    "    k_neighbors=5,\n",
    "    m_neighbors=10,\n",
    "    kind='borderline-1',\n",
    "    n_jobs=4\n",
    ")\n",
    "\n",
    "# Resample the data\n",
    "X_res, y_res = ada.fit_resample(X, Y)\n",
    "\n",
    "# Print the shapes of resampled data\n",
    "print(X_res.shape, y_res.shape)\n",
    "\n",
    "# Print the counts of the original and resampled target variable\n",
    "print(Y.value_counts(), y_res.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# separate dataset into train and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=0)\n",
    "\n",
    "# Display shapes of train and test sets\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "# Display first few rows of test set\n",
    "print(X_test.head())\n",
    "\n",
    "# Display normalized value counts of target variable in test set\n",
    "print(y_test.value_counts(normalize=True))\n",
    "\n",
    "# Keep a copy of the original datasets\n",
    "X_train_original = X_train.copy()\n",
    "X_test_original = X_test.copy()\n",
    "\n",
    "# Check shapes of train and test sets after copying\n",
    "print(X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = X_train_original\n",
    "#X_test = X_test_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove constant and quasi-constant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove constant features\n",
    "sel = VarianceThreshold(threshold=0)\n",
    "sel.fit(X_train)\n",
    "\n",
    "# Get the number of features that are not constant\n",
    "num_non_constant_features = sum(sel.get_support())\n",
    "\n",
    "# Remove quasi-constant features\n",
    "sel = VarianceThreshold(threshold=0.1)  # 0.1 indicates 99% of observations approximately\n",
    "sel.fit(X_train)\n",
    "num_non_quasi_constant_features = sum(sel.get_support())\n",
    "\n",
    "# Get constant feature names\n",
    "constant_features = X_train.columns[~sel.get_support()]\n",
    "\n",
    "# Get non-constant feature names\n",
    "non_constant_feature_names = X_train.columns[sel.get_support()]\n",
    "\n",
    "# Transform train and test sets\n",
    "X_train = sel.transform(X_train)\n",
    "X_test = sel.transform(X_test)\n",
    "\n",
    "# Convert arrays back to DataFrame with non-constant features\n",
    "X_train = pd.DataFrame(X_train, index=X_train_original.index, columns=non_constant_feature_names)\n",
    "X_test = pd.DataFrame(X_test, index=X_test_original.index, columns=non_constant_feature_names)\n",
    "\n",
    "# Print shapes of transformed data\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "# Display first few rows of transformed data\n",
    "print(X_train.head())\n",
    "print(X_test.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate-Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.selection import SelectBySingleFeaturePerformance\n",
    "\n",
    "# Define the estimator\n",
    "rf = ExtraTreesClassifier()\n",
    "\n",
    "# Set up the selector\n",
    "sel = SelectBySingleFeaturePerformance(\n",
    "    variables=None,  # Select all features\n",
    "    estimator=rf,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=3,\n",
    "    threshold=0.70,\n",
    ")\n",
    "\n",
    "# Find predictive features\n",
    "sel.fit(X_train, y_train)\n",
    "\n",
    "# Get feature performance\n",
    "feature_performance = sel.feature_performance_\n",
    "\n",
    "# Get the number of features to be removed\n",
    "num_features_to_drop = len(sel.features_to_drop_)\n",
    "\n",
    "# Remove non-predictive features\n",
    "X_train = sel.transform(X_train)\n",
    "X_test = sel.transform(X_test)\n",
    "\n",
    "# Print shapes of transformed data\n",
    "print(X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_uni = X_train\n",
    "X_test_uni = X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# relieff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrebate import ReliefF\n",
    "fs = ReliefF()\n",
    "fs.fit(X_train.values, y_train.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "accuracy_rate = []\n",
    "clf1 = ExtraTreesClassifier()\n",
    "clf2 = LogisticRegression(penalty ='l2', C=0.1, solver='liblinear', random_state=4,max_iter = 1000)\n",
    "clf3 = SVC(gamma =1e-05, probability = True, decision_function_shape = 'ovo', kernel = 'linear')\n",
    "clf4 = AdaBoostClassifier()\n",
    "clf5= XGBClassifier()\n",
    "\n",
    "\n",
    "# Will take some time\n",
    "for i in range(5,305,5):\n",
    "    \n",
    "    indx_sort=fs.feature_importances_.argsort()[-i:][::-1]\n",
    "    X_train_rf= X_train.iloc[:,indx_sort]\n",
    "    X_test_rf= X_test.iloc[:,indx_sort]\n",
    "    \n",
    "    eclf1 = VotingClassifier(estimators=[ ('logistic', clf2),('SVM', clf3),\n",
    "                                           ('XGB', clf5), \n",
    "                                          ('extratree',clf1), ('AdaBoost', clf4)\n",
    "                                        ], voting='hard') \n",
    "    \n",
    "   \n",
    "    print(i)\n",
    "    \n",
    "    \n",
    "    for clf in (clf1,clf2, clf3, clf4, clf5, eclf1):\n",
    "        clf.fit(X_train_rf, y_train)\n",
    "        y_pred = clf.predict(X_test_rf)\n",
    "        print(clf.__class__.__name__, round(accuracy_score(y_test, y_pred),3))\n",
    "        \n",
    "  \n",
    "\n",
    "\n",
    "    # Number of iterations\n",
    "    num_iterations = 3\n",
    "\n",
    "    # Initialize a list to store accuracy scores\n",
    "    ensemble_scores = []\n",
    "\n",
    "    # Perform multiple iterations of the VotingClassifier\n",
    "    for _ in range(num_iterations):\n",
    "        eclf1.fit(X_train_rf, y_train)\n",
    "        predictions = eclf1.predict(X_test_rf)\n",
    "        ensemble_acc = accuracy_score(y_test, predictions)\n",
    "        ensemble_scores.append(ensemble_acc)\n",
    "\n",
    "    print(\"Ensemble Voting Accuracy:\", ensemble_scores)\n",
    "    \n",
    "    # Calculate the mean accuracy of the ensemble\n",
    "    mean_ensemble_accuracy = np.mean(ensemble_scores)\n",
    "    print(\"Mean Ensemble Voting Accuracy:\", mean_ensemble_accuracy)\n",
    "\n",
    "    print(X_train_rf.columns)\n",
    "    accuracy_rate.append(mean_ensemble_accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(range(5,300,5),accuracy_rate,color='blue', linestyle='solid', marker='o',\n",
    "         markerfacecolor='blue', markersize=10)\n",
    "\n",
    "title =plt.title('ReliefF Accuracy vs. No. of Genes', fontsize=20)\n",
    "plt.xlabel('No. of Genes', fontsize=16)\n",
    "plt.ylabel('ReliefF Accuracy', fontsize=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract top 95 features ranked by Relieff, as obtainted in the increment feature selection procedure above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx_sort=fs.feature_importances_.argsort()[-95:][::-1]\n",
    "indx_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rf= X_train.iloc[:,indx_sort]\n",
    "X_train_rf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_rf= X_test.iloc[:,indx_sort]\n",
    "X_test_rf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how much immune related genes in the selected genes\n",
    "IRG_feat=X_train_rf.columns.intersection(IRG['Symbol'])\n",
    "IRG_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_rf\n",
    "X_test= X_test_rf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# draw figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(range(5,305,5),accuracy_plot['ReliefF_accuracy'],color='blue', linestyle='solid', marker='o',\n",
    "         markerfacecolor='blue', markersize=10)\n",
    "\n",
    "\n",
    "title =plt.title('Uni + ReliefF Accuracy vs. No. of Genes', fontsize=20)\n",
    "plt.xlabel('No. of Genes', fontsize=16)\n",
    "plt.ylabel('ReliefF Accuracy', fontsize=16)\n",
    "#plt.legend()\n",
    "plt.savefig('gide_accuracy_plot_uni relieff.jpg')\n",
    "plt.savefig('gide_accuracy_plot_uni relieff.eps',dpi=300)\n",
    "plt.savefig('gide_accuracy_plot_uni relieff.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM-RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "accuracy_rate = []\n",
    "clf1 = ExtraTreesClassifier()\n",
    "clf2 = LogisticRegression(penalty ='l2', C=0.1, solver='liblinear', random_state=4,max_iter = 1000)\n",
    "clf3 = SVC(gamma =1e-05, probability = True, decision_function_shape = 'ovo', kernel = 'linear')\n",
    "clf4 = AdaBoostClassifier()\n",
    "clf5= XGBClassifier()\n",
    "\n",
    "\n",
    "\n",
    "# Will take some time\n",
    "for i in range(1, 51, 1):\n",
    "    \n",
    "    svm= SVC(gamma =1e-05, probability = True, decision_function_shape = 'ovo', kernel = 'linear')\n",
    "    sel_ = RFE(svm, n_features_to_select=i)\n",
    "    sel_.fit(X_train, y_train)\n",
    "    selected_feat = X_train.columns[(sel_.get_support())]\n",
    "    print(len(selected_feat))\n",
    "    X_train_rf = X_train[selected_feat]\n",
    "    X_test_rf = X_test[selected_feat]\n",
    "    # display features\n",
    "    \n",
    "    \n",
    "    \n",
    "    eclf1 = VotingClassifier(estimators=[ ('logistic', clf2),('SVM', clf3),\n",
    "                                           ('XGB', clf5), \n",
    "                                          ('extratree',clf1), ('AdaBoost', clf4)\n",
    "                                        ], voting='hard')    \n",
    "    print(i)\n",
    "    \n",
    "    for clf in (clf1,clf2, clf3, clf4, clf5, eclf1):\n",
    "        clf.fit(X_train_rf, y_train)\n",
    "        y_pred = clf.predict(X_test_rf)\n",
    "        print(clf.__class__.__name__, round(accuracy_score(y_test, y_pred),3))\n",
    "    \n",
    "    eclf1.fit(X_train_rf, y_train)\n",
    "    predictions = eclf1.predict(X_test_rf)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(X_train_rf.columns)\n",
    "    accuracy_rate.append(round(accuracy_score(y_test, predictions),3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(range(1,51,1),accuracy_rate,color='blue', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='red', markersize=10)\n",
    "plt.title('uni+ relieff + SVM-RFE')\n",
    "plt.xlabel('No. of Genes')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the best set of genes by SVM-RFE, as obtainted in the increment feature selection procedure above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "sel_ = RFE(SVC(gamma =1e-05, probability = True, decision_function_shape = 'ovo', kernel = 'linear'), n_features_to_select=35)\n",
    "sel_.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feat = X_train.columns[(sel_.get_support())]\n",
    "len(selected_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rf = X_train[selected_feat]\n",
    "X_test_rf = X_test[selected_feat]\n",
    "X_train_rf.shape, X_test_rf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_rf\n",
    "X_test= X_test_rf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the reduced data with the final selected genes only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train_selected = pd.DataFrame(X_train_original,index= X_train_original.index, columns= X_train_rf.columns)\n",
    "X_train_selected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_test_selected = pd.DataFrame(X_test_original,index= X_test_original.index, columns= X_train_rf.columns)\n",
    "X_test_selected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train =X_train_selected\n",
    "X_test = X_test_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "clf1 = ExtraTreesClassifier()\n",
    "clf2 = LogisticRegression(penalty ='l2', C=0.1, solver='liblinear', random_state=4,max_iter = 1000)\n",
    "clf3 = SVC(gamma =1e-05, probability = True, decision_function_shape = 'ovo', kernel = 'linear')\n",
    "clf4 = AdaBoostClassifier()\n",
    "clf5= XGBClassifier()            \n",
    "               \n",
    "\n",
    "\n",
    "df_test = pd.DataFrame()\n",
    "df_test['y_test'] = y_test\n",
    "for clf in (clf1,clf2, clf3, clf4, clf5):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    df_test[clf.__class__.__name__] = round(accuracy_score(y_test, y_pred),3)\n",
    "    print(clf.__class__.__name__, round(accuracy_score(y_test, y_pred),3))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf1 = ExtraTreesClassifier()\n",
    "clf2 = LogisticRegression(penalty ='l2', C=0.1, solver='liblinear', random_state=4,max_iter = 1000)\n",
    "clf3 = SVC(gamma =1e-05, probability = True, decision_function_shape = 'ovo', kernel = 'linear')\n",
    "clf4 = AdaBoostClassifier()\n",
    "clf5= XGBClassifier()            \n",
    "               \n",
    "eclf1 = VotingClassifier(estimators=[  ('logistic', clf2),('SVM', clf3),\n",
    "                                        ('XGB', clf5), ('extratree',clf1), \n",
    "                                        ('AdaBoost', clf4)\n",
    "                                        ], voting='hard') \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for clf in (clf1,clf2, clf3, clf4, clf5, eclf1):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, round(accuracy_score(y_test, y_pred),3))\n",
    "\n",
    "\n",
    "\n",
    "eclf1.fit(X_train, y_train)\n",
    "predictions = eclf1.predict(X_test)\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "confusion_mat = confusion_matrix(y_test, predictions)\n",
    "# Extract values from the confusion matrix\n",
    "true_positive = confusion_mat[1, 1]\n",
    "false_positive = confusion_mat[0, 1]\n",
    "true_negative = confusion_mat[0, 0]\n",
    "false_negative = confusion_mat[1, 0]\n",
    "\n",
    "# Calculate Sensitivity (True Positive Rate)\n",
    "sensitivity = true_positive / (true_positive + false_negative)\n",
    "\n",
    "# Calculate Specificity\n",
    "specificity = true_negative / (true_negative + false_positive)\n",
    "\n",
    "# Print the results\n",
    "print(f'Sensitivity (True Positive Rate): {sensitivity:.2f}')\n",
    "print(f'Specificity: {specificity:.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
