{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this code for compare diffrent types of feature selection techniques.\n",
    "experiment 3: Comparison between the probosed hybrid model (univariate ROC-AUC + ReliefF + SVM-RFE, and three different combi- nations of feature selection techniques (Mutual+ SVM-RFE. ReliefF+ SVM-RFE, and Uni+ SVM-RFE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split, LeaveOneOut, KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, classification_report\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "import xgboost as xgb\n",
    "from feature_engine.selection import DropDuplicateFeatures, DropConstantFeatures\n",
    "from sklearn.feature_selection import SelectFromModel, VarianceThreshold\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN, BorderlineSMOTE, SVMSMOTE\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import roc_auc_score, plot_roc_curve\n",
    "from yellowbrick.classifier import ROCAUC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input the data (gene expression array) and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['riaz', 'gide', 'van allen', 'hugo', 'lee']\n",
    "name= filenames[1]\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['riaz', 'gide', 'van allen', 'hugo', 'lee']\n",
    "name= filenames[1]\n",
    "\n",
    "# Read logged data\n",
    "logged_data = pd.read_csv('data/' + name + '_logged_data.csv', index_col='Unnamed: 0').T\n",
    "print(logged_data.shape)\n",
    "print(logged_data.head())\n",
    "\n",
    "# Read labels\n",
    "label = pd.read_csv('data/' + name + '.Pre.Samples.corr.csv', index_col='Unnamed: 0')\n",
    "print(label.shape)\n",
    "print(label.head())\n",
    "\n",
    "# Count of labels\n",
    "print(label['Resp_NoResp'].value_counts())\n",
    "\n",
    "# Normalized count of labels\n",
    "print(label['Resp_NoResp'].value_counts(normalize=True))\n",
    "\n",
    "# Label encoding\n",
    "y = label.replace({'No_Response': 0, 'Response': 1})\n",
    "labels = ['No_Response', 'Response']  # for plotting convenience later on\n",
    "\n",
    "print(y.head())\n",
    "print(y['Resp_NoResp'].head())\n",
    "print(y.iloc[:, 0].head())\n",
    "print(y.iloc[:, 0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the immune related genes from immport\n",
    "IRG= pd.read_csv('IRG.csv')\n",
    "print(IRG.shape)\n",
    "IRG.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (Y)\n",
    "X = logged_data\n",
    "Y = y.iloc[:, 0]\n",
    "\n",
    "# Oversampling using BorderlineSMOTE\n",
    "ada = BorderlineSMOTE(\n",
    "    sampling_strategy='auto',  # samples only the minority class\n",
    "    random_state=0,  # for reproducibility\n",
    "    k_neighbors=5,\n",
    "    m_neighbors=10,\n",
    "    kind='borderline-1',\n",
    "    n_jobs=4\n",
    ")\n",
    "\n",
    "# Resample the data\n",
    "X_res, y_res = ada.fit_resample(X, Y)\n",
    "\n",
    "# Print the shapes of resampled data\n",
    "print(X_res.shape, y_res.shape)\n",
    "\n",
    "# Print the counts of the original and resampled target variable\n",
    "print(Y.value_counts(), y_res.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# separate dataset into train and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=0)\n",
    "\n",
    "# Display shapes of train and test sets\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "# Display first few rows of test set\n",
    "print(X_test.head())\n",
    "\n",
    "# Display normalized value counts of target variable in test set\n",
    "print(y_test.value_counts(normalize=True))\n",
    "\n",
    "# Keep a copy of the original datasets\n",
    "X_train_original = X_train.copy()\n",
    "X_test_original = X_test.copy()\n",
    "\n",
    "# Check shapes of train and test sets after copying\n",
    "print(X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# result of the probosed hybrid model (univariate ROC-AUC + ReliefF + SVM-RFE), use of the data selected by Univariate ROC-AUC Performance + ReliefF from experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('gide_X_train_uni_reliefF.csv',index_col = 'Unnamed: 0')\n",
    "print(X_train.shape)\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('gide_X_test_uni_reliefF.csv',index_col = 'Unnamed: 0')\n",
    "print(X_test.shape)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM-RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "accuracy_rate = []\n",
    "clf1 = ExtraTreesClassifier()\n",
    "clf2 = LogisticRegression(penalty ='l2', C=0.1, solver='liblinear', random_state=4,max_iter = 1000)\n",
    "clf3 = SVC(gamma =1e-05, probability = True, decision_function_shape = 'ovo', kernel = 'linear')\n",
    "clf4 = AdaBoostClassifier()\n",
    "clf5= XGBClassifier()\n",
    "\n",
    "\n",
    "\n",
    "# Will take some time\n",
    "for i in range(1, 51, 1):\n",
    "    \n",
    "    svm= SVC(gamma =1e-05, probability = True, decision_function_shape = 'ovo', kernel = 'linear')\n",
    "    sel_ = RFE(svm, n_features_to_select=i)\n",
    "    sel_.fit(X_train, y_train)\n",
    "    selected_feat = X_train.columns[(sel_.get_support())]\n",
    "    print(len(selected_feat))\n",
    "    X_train_rfe = X_train[selected_feat]\n",
    "    X_test_rfe = X_test[selected_feat]\n",
    "    # display features\n",
    "    \n",
    "    \n",
    "    \n",
    "    eclf1 = VotingClassifier(estimators=[ ('logistic', clf2),('SVM', clf3),\n",
    "                                           ('XGB', clf5), \n",
    "                                          ('extratree',clf1), ('AdaBoost', clf4)\n",
    "                                        ], voting='hard')    \n",
    "    print(i)\n",
    "    \n",
    "    for clf in (clf1,clf2, clf3, clf4, clf5, eclf1):\n",
    "        clf.fit(X_train_rfe, y_train)\n",
    "        y_pred = clf.predict(X_test_rfe)\n",
    "        print(clf.__class__.__name__, round(accuracy_score(y_test, y_pred),3))\n",
    "    \n",
    "    eclf1.fit(X_train_rfe, y_train)\n",
    "    predictions = eclf1.predict(X_test_rfe)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(X_train_rfe.columns)\n",
    "    accuracy_rate.append(round(accuracy_score(y_test, predictions),3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(range(1,51,1),accuracy_rate,color='blue', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='red', markersize=10)\n",
    "plt.title('uni+ relieff + SVM-RFE')\n",
    "plt.xlabel('No. of Genes')\n",
    "plt.ylabel('ReliefF Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save result in a data frame for ploting\n",
    "accuracy_plot_new = pd.DataFrame()\n",
    "accuracy_plot_new['uni+ relieff + RFE'] = accuracy_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# result of the (univariate ROC-AUC + SVM-RFE) model, use of the data selected by Univariate ROC-AUC Performance from experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('gide_X_train_uni_roc-auc.csv',index_col = 'Unnamed: 0')\n",
    "print(X_train.shape)\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('gide_X_test_uni_roc-auc.csv',index_col = 'Unnamed: 0')\n",
    "print(X_test.shape)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM-RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "accuracy_rate = []\n",
    "clf1 = ExtraTreesClassifier()\n",
    "clf2 = LogisticRegression(penalty ='l2', C=0.1, solver='liblinear', random_state=4,max_iter = 1000)\n",
    "clf3 = SVC(gamma =1e-05, probability = True, decision_function_shape = 'ovo', kernel = 'linear')\n",
    "clf4 = AdaBoostClassifier()\n",
    "clf5= XGBClassifier()\n",
    "\n",
    "\n",
    "\n",
    "# Will take some time\n",
    "for i in range(1, 51, 1):\n",
    "    \n",
    "    svm= SVC(gamma =1e-05, probability = True, decision_function_shape = 'ovo', kernel = 'linear')\n",
    "    sel_ = RFE(svm, n_features_to_select=i)\n",
    "    sel_.fit(X_train, y_train)\n",
    "    selected_feat = X_train.columns[(sel_.get_support())]\n",
    "    print(len(selected_feat))\n",
    "    X_train_uni_rfe = X_train[selected_feat]\n",
    "    X_test_uni_rfe = X_test[selected_feat]\n",
    "    # display features\n",
    "    \n",
    "    \n",
    "    \n",
    "    eclf1 = VotingClassifier(estimators=[ ('logistic', clf2),('SVM', clf3),\n",
    "                                           ('XGB', clf5), \n",
    "                                          ('extratree',clf1), ('AdaBoost', clf4)\n",
    "                                        ], voting='hard')    \n",
    "    print(i)\n",
    "    \n",
    "    for clf in (clf1,clf2, clf3, clf4, clf5, eclf1):\n",
    "        clf.fit(X_train_uni_rfe, y_train)\n",
    "        y_pred = clf.predict(X_test_uni_rfe)\n",
    "        print(clf.__class__.__name__, round(accuracy_score(y_test, y_pred),3))\n",
    "    \n",
    "    eclf1.fit(X_train_uni_rfe, y_train)\n",
    "    predictions = eclf1.predict(X_test_uni_rfe)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(X_train_uni_rfe.columns)\n",
    "    accuracy_rate.append(round(accuracy_score(y_test, predictions),3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(range(1,51,1),accuracy_rate,color='blue', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='red', markersize=10)\n",
    "plt.title('uni + SVM-RFE')\n",
    "plt.xlabel('No. of Genes')\n",
    "plt.ylabel('ReliefF Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save result in a data frame 'accuracy_plot_new'\n",
    "\n",
    "accuracy_plot_new['uni + RFE'] = accuracy_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# result of the (Relieff + SVM-RFE) model, use of the data selected by Relieff from experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('gide_X_train_reliefF.csv',index_col = 'Unnamed: 0')\n",
    "print(X_train.shape)\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('gide_X_test_reliefF.csv',index_col = 'Unnamed: 0')\n",
    "print(X_test.shape)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM-RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "accuracy_rate = []\n",
    "clf1 = ExtraTreesClassifier()\n",
    "clf2 = LogisticRegression(penalty ='l2', C=0.1, solver='liblinear', random_state=4,max_iter = 1000)\n",
    "clf3 = SVC(gamma =1e-05, probability = True, decision_function_shape = 'ovo', kernel = 'linear')\n",
    "clf4 = AdaBoostClassifier()\n",
    "clf5= XGBClassifier()\n",
    "\n",
    "\n",
    "\n",
    "# Will take some time\n",
    "for i in range(1, 51, 1):\n",
    "    \n",
    "    svm= SVC(gamma =1e-05, probability = True, decision_function_shape = 'ovo', kernel = 'linear')\n",
    "    sel_ = RFE(svm, n_features_to_select=i)\n",
    "    sel_.fit(X_train, y_train)\n",
    "    selected_feat = X_train.columns[(sel_.get_support())]\n",
    "    print(len(selected_feat))\n",
    "    X_train_rf_rfe = X_train[selected_feat]\n",
    "    X_test_rf_rfe = X_test[selected_feat]\n",
    "    # display features\n",
    "    \n",
    "    \n",
    "    \n",
    "    eclf1 = VotingClassifier(estimators=[ ('logistic', clf2),('SVM', clf3),\n",
    "                                           ('XGB', clf5), \n",
    "                                          ('extratree',clf1), ('AdaBoost', clf4)\n",
    "                                        ], voting='hard')    \n",
    "    print(i)\n",
    "    \n",
    "    for clf in (clf1,clf2, clf3, clf4, clf5, eclf1):\n",
    "        clf.fit(X_train_rf_rfe, y_train)\n",
    "        y_pred = clf.predict(X_test_rf_rfe)\n",
    "        print(clf.__class__.__name__, round(accuracy_score(y_test, y_pred),3))\n",
    "    \n",
    "    eclf1.fit(X_train_rf_rfe, y_train)\n",
    "    predictions = eclf1.predict(X_test_rf_rfe)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(X_train_rf_rfe.columns)\n",
    "    accuracy_rate.append(round(accuracy_score(y_test, predictions),3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(range(1,51,1),accuracy_rate,color='blue', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='red', markersize=10)\n",
    "plt.title(' relieff + SVM-RFE')\n",
    "plt.xlabel('No. of Genes')\n",
    "plt.ylabel('ReliefF Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save result in a data frame 'accuracy_plot_new'\n",
    "\n",
    "accuracy_plot_new['relieff + RFE'] = accuracy_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# result of the (Mutual information + SVM-RFE) model, use of the data selected by Relieff from experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('gide_X_train_MI.csv',index_col = 'Unnamed: 0')\n",
    "print(X_train.shape)\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('gide_X_test_MI.csv',index_col = 'Unnamed: 0')\n",
    "print(X_test.shape)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM-RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "accuracy_rate = []\n",
    "clf1 = ExtraTreesClassifier()\n",
    "clf2 = LogisticRegression(penalty ='l2', C=0.1, solver='liblinear', random_state=4,max_iter = 1000)\n",
    "clf3 = SVC(gamma =1e-05, probability = True, decision_function_shape = 'ovo', kernel = 'linear')\n",
    "clf4 = AdaBoostClassifier()\n",
    "clf5= XGBClassifier()\n",
    "\n",
    "\n",
    "\n",
    "# Will take some time\n",
    "for i in range(1, 51, 1):\n",
    "    \n",
    "    svm= SVC(gamma =1e-05, probability = True, decision_function_shape = 'ovo', kernel = 'linear')\n",
    "    sel_ = RFE(svm, n_features_to_select=i)\n",
    "    sel_.fit(X_train, y_train)\n",
    "    selected_feat = X_train.columns[(sel_.get_support())]\n",
    "    print(len(selected_feat))\n",
    "    X_train_mi_rfe = X_train[selected_feat]\n",
    "    X_test_mi_rfe = X_test[selected_feat]\n",
    "    # display features\n",
    "    \n",
    "    \n",
    "    \n",
    "    eclf1 = VotingClassifier(estimators=[ ('logistic', clf2),('SVM', clf3),\n",
    "                                           ('XGB', clf5), \n",
    "                                          ('extratree',clf1), ('AdaBoost', clf4)\n",
    "                                        ], voting='hard')    \n",
    "    print(i)\n",
    "    \n",
    "    for clf in (clf1,clf2, clf3, clf4, clf5, eclf1):\n",
    "        clf.fit(X_train_mi_rfe, y_train)\n",
    "        y_pred = clf.predict(X_test_mi_rfe)\n",
    "        print(clf.__class__.__name__, round(accuracy_score(y_test, y_pred),3))\n",
    "    \n",
    "    eclf1.fit(X_train_mi_rfe, y_train)\n",
    "    predictions = eclf1.predict(X_test_mi_rfe)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(X_train_mi_rfe.columns)\n",
    "    accuracy_rate.append(round(accuracy_score(y_test, predictions),3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(range(1,51,1),accuracy_rate,color='blue', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='red', markersize=10)\n",
    "plt.title(' relieff + SVM-RFE')\n",
    "plt.xlabel('No. of Genes')\n",
    "plt.ylabel('ReliefF Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save result in a data frame 'accuracy_plot_new'\n",
    "\n",
    "accuracy_plot_new['mutual + RFE'] = accuracy_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(range(1,51,1),accuracy_plot_new['mutual + RFE'],color='blue', linestyle='solid', marker='o',\n",
    "         markerfacecolor='blue', markersize=5, label = \"mutual + SVM-RFE\")\n",
    "plt.plot(range(1,51,1),accuracy_plot_new['uni + RFE'],color='green', linestyle='solid',  marker='o',\n",
    "         markerfacecolor='green', markersize=5, label = \"uni + SVM-RFE\")\n",
    "plt.plot(range(1,51,1),accuracy_plot_new['relieff + RFE'],color='red', linestyle='solid', marker='o',\n",
    "         markerfacecolor='red', markersize=5, label = \"reliefF + SVM-RFE\")\n",
    "plt.plot(range(1,51,1),accuracy_plot_new['uni+ relieff + RFE'],color='black', linestyle='solid', marker='o',\n",
    "         markerfacecolor='black', markersize=7, label = \"uni+ reliefF + SVM-RFE\")\n",
    "\n",
    "#plt.ylim(0.4, 1)\n",
    "#plt.title('ReliefF and mutual info Accuracies vs. No. of Genes')\n",
    "plt.xlabel('No. of Genes', fontsize=16)\n",
    "plt.ylabel(' Accuracy', fontsize=16)\n",
    "plt.legend()\n",
    "plt.gca().invert_xaxis()\n",
    "plt.savefig('four curves_gide.jpg')\n",
    "plt.savefig('four curves_gide.png')\n",
    "plt.savefig('four curves_gide.eps',dpi=300)\n",
    "plt.savefig('four curves_gide.pdf',dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
