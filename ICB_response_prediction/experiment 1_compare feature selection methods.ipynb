{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this code for compare diffrent types of feature selection.\n",
    "experiment 1: compare four filter feature selection methods widely used with gene expression data, namely, univariate ROC-AUC performance, ReliefF, mutual information, and ANOVA. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split, LeaveOneOut, KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, classification_report\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "import xgboost as xgb\n",
    "from feature_engine.selection import DropDuplicateFeatures, DropConstantFeatures\n",
    "from sklearn.feature_selection import SelectFromModel, VarianceThreshold\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN, BorderlineSMOTE, SVMSMOTE\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import roc_auc_score, plot_roc_curve\n",
    "from yellowbrick.classifier import ROCAUC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input the data (gene expression array) and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['riaz', 'gide', 'van allen', 'hugo', 'lee']\n",
    "name= filenames[1]\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['riaz', 'gide', 'van allen', 'hugo', 'lee']\n",
    "name= filenames[1]\n",
    "\n",
    "# Read logged data\n",
    "logged_data = pd.read_csv('data/' + name + '_logged_data.csv', index_col='Unnamed: 0').T\n",
    "print(logged_data.shape)\n",
    "print(logged_data.head())\n",
    "\n",
    "# Read labels\n",
    "label = pd.read_csv('data/' + name + '.Pre.Samples.corr.csv', index_col='Unnamed: 0')\n",
    "print(label.shape)\n",
    "print(label.head())\n",
    "\n",
    "# Count of labels\n",
    "print(label['Resp_NoResp'].value_counts())\n",
    "\n",
    "# Normalized count of labels\n",
    "print(label['Resp_NoResp'].value_counts(normalize=True))\n",
    "\n",
    "# Label encoding\n",
    "y = label.replace({'No_Response': 0, 'Response': 1})\n",
    "labels = ['No_Response', 'Response']  # for plotting convenience later on\n",
    "\n",
    "print(y.head())\n",
    "print(y['Resp_NoResp'].head())\n",
    "print(y.iloc[:, 0].head())\n",
    "print(y.iloc[:, 0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the immune related genes from immport\n",
    "IRG= pd.read_csv('IRG.csv')\n",
    "print(IRG.shape)\n",
    "IRG.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (Y)\n",
    "X = logged_data\n",
    "Y = y.iloc[:, 0]\n",
    "\n",
    "# Oversampling using BorderlineSMOTE\n",
    "ada = BorderlineSMOTE(\n",
    "    sampling_strategy='auto',  # samples only the minority class\n",
    "    random_state=0,  # for reproducibility\n",
    "    k_neighbors=5,\n",
    "    m_neighbors=10,\n",
    "    kind='borderline-1',\n",
    "    n_jobs=4\n",
    ")\n",
    "\n",
    "# Resample the data\n",
    "X_res, y_res = ada.fit_resample(X, Y)\n",
    "\n",
    "# Print the shapes of resampled data\n",
    "print(X_res.shape, y_res.shape)\n",
    "\n",
    "# Print the counts of the original and resampled target variable\n",
    "print(Y.value_counts(), y_res.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# separate dataset into train and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=0)\n",
    "\n",
    "# Display shapes of train and test sets\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "# Display first few rows of test set\n",
    "print(X_test.head())\n",
    "\n",
    "# Display normalized value counts of target variable in test set\n",
    "print(y_test.value_counts(normalize=True))\n",
    "\n",
    "# Keep a copy of the original datasets\n",
    "X_train_original = X_train.copy()\n",
    "X_test_original = X_test.copy()\n",
    "\n",
    "# Check shapes of train and test sets after copying\n",
    "print(X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove constant and quasi-constant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove constant features\n",
    "sel = VarianceThreshold(threshold=0)\n",
    "sel.fit(X_train)\n",
    "\n",
    "# Get the number of features that are not constant\n",
    "num_non_constant_features = sum(sel.get_support())\n",
    "\n",
    "# Remove quasi-constant features\n",
    "sel = VarianceThreshold(threshold=0.1)  # 0.1 indicates 99% of observations approximately\n",
    "sel.fit(X_train)\n",
    "num_non_quasi_constant_features = sum(sel.get_support())\n",
    "\n",
    "# Get constant feature names\n",
    "constant_features = X_train.columns[~sel.get_support()]\n",
    "\n",
    "# Get non-constant feature names\n",
    "non_constant_feature_names = X_train.columns[sel.get_support()]\n",
    "\n",
    "# Transform train and test sets\n",
    "X_train = sel.transform(X_train)\n",
    "X_test = sel.transform(X_test)\n",
    "\n",
    "# Convert arrays back to DataFrame with non-constant features\n",
    "X_train = pd.DataFrame(X_train, index=X_train_original.index, columns=non_constant_feature_names)\n",
    "X_test = pd.DataFrame(X_test, index=X_test_original.index, columns=non_constant_feature_names)\n",
    "\n",
    "# Display first few rows of transformed data\n",
    "print(X_train.head())\n",
    "print(X_test.head())\n",
    "\n",
    "# Print shapes of transformed data\n",
    "print(X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep a copy of the filtered datasets\n",
    "X_train_filtered = X_train.copy()\n",
    "X_test_filtered = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_filtered.shape, X_test_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experiment 1: Comparison between four different filter feature selection methods univariate ROC-AUC, ReliefF, mutual information, and ANOVA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate ROC-AUC Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.selection import SelectBySingleFeaturePerformance\n",
    "\n",
    "# Define the estimator\n",
    "rf = ExtraTreesClassifier()\n",
    "\n",
    "# Set up the selector\n",
    "sel = SelectBySingleFeaturePerformance(\n",
    "    variables=None,  # Select all features\n",
    "    estimator=rf,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=3,\n",
    "    threshold=0.70,\n",
    ")\n",
    "\n",
    "# Find predictive features\n",
    "sel.fit(X_train, y_train)\n",
    "\n",
    "# Get feature performance\n",
    "feature_performance = sel.feature_performance_\n",
    "\n",
    "# Get the number of features to be removed\n",
    "num_features_to_drop = len(sel.features_to_drop_)\n",
    "\n",
    "# Remove non-predictive features\n",
    "X_train = sel.transform(X_train)\n",
    "X_test = sel.transform(X_test)\n",
    "\n",
    "# Print shapes of transformed data\n",
    "print(X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep acopy of the data selected by Univariate ROC-AUC Performance\n",
    "X_train_uni = X_train\n",
    "X_test_uni = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_uni.shape, X_test_uni.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save a copy of univariate roc-auc result for the following experiment \n",
    "X_train.to_csv('gide_X_train_uni_roc-auc.csv')\n",
    "X_test.to_csv('gide_X_test_uni_roc-auc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# univariate performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf1 = ExtraTreesClassifier()\n",
    "clf2 = LogisticRegression(penalty ='l1', C=0.1, solver='liblinear', random_state=4,max_iter = 1000)\n",
    "clf3 = SVC(gamma =1e-05, probability = True, decision_function_shape = 'ovo', kernel = 'linear')\n",
    "clf4 = AdaBoostClassifier()\n",
    "clf5= XGBClassifier()            \n",
    "               \n",
    "eclf1 = VotingClassifier(estimators=[  ('logistic', clf2),('SVM', clf3),\n",
    "                                        ('XGB', clf5), ('extratree',clf1), \n",
    "                                        ('AdaBoost', clf4)\n",
    "                                        ], voting='hard') \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for clf in (clf1,clf2, clf3, clf4, clf5, eclf1):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, round(accuracy_score(y_test, y_pred),3))\n",
    "\n",
    "\n",
    "\n",
    "eclf1.fit(X_train, y_train)\n",
    "predictions = eclf1.predict(X_test)\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "confusion_mat = confusion_matrix(y_test, predictions)\n",
    "# Extract values from the confusion matrix\n",
    "true_positive = confusion_mat[1, 1]\n",
    "false_positive = confusion_mat[0, 1]\n",
    "true_negative = confusion_mat[0, 0]\n",
    "false_negative = confusion_mat[1, 0]\n",
    "\n",
    "# Calculate Sensitivity (True Positive Rate)\n",
    "sensitivity = true_positive / (true_positive + false_negative)\n",
    "\n",
    "# Calculate Specificity\n",
    "specificity = true_negative / (true_negative + false_positive)\n",
    "\n",
    "# Print the results\n",
    "print(f'Sensitivity (True Positive Rate): {sensitivity:.2f}')\n",
    "print(f'Specificity: {specificity:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# relieff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return x_train & x_test to the filtered values.\n",
    "X_train = X_train_filtered\n",
    "X_test = X_test_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrebate import ReliefF\n",
    "fs = ReliefF()\n",
    "fs.fit(X_train.values, y_train.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "accuracy_rate = []\n",
    "clf1 = ExtraTreesClassifier()\n",
    "clf2 = LogisticRegression(penalty ='l2', C=0.1, solver='liblinear', random_state=4,max_iter = 1000)\n",
    "clf3 = SVC(gamma =1e-05, probability = True, decision_function_shape = 'ovo', kernel = 'linear')\n",
    "clf4 = AdaBoostClassifier()\n",
    "clf5= XGBClassifier()\n",
    "\n",
    "\n",
    "# Will take some time\n",
    "for i in range(500,8500,500):\n",
    "    \n",
    "    indx_sort=fs.feature_importances_.argsort()[-i:][::-1]\n",
    "    X_train_rf= X_train.iloc[:,indx_sort]\n",
    "    X_test_rf= X_test.iloc[:,indx_sort]\n",
    "    \n",
    "    eclf1 = VotingClassifier(estimators=[ ('logistic', clf2),('SVM', clf3),\n",
    "                                           ('XGB', clf5), \n",
    "                                          ('extratree',clf1), ('AdaBoost', clf4)\n",
    "                                        ], voting='hard') \n",
    "    \n",
    "   \n",
    "    print(i)\n",
    "    \n",
    "    \n",
    "    for clf in (clf1,clf2, clf3, clf4, clf5, eclf1):\n",
    "        clf.fit(X_train_rf, y_train)\n",
    "        y_pred = clf.predict(X_test_rf)\n",
    "        print(clf.__class__.__name__, round(accuracy_score(y_test, y_pred),3))\n",
    "        \n",
    "  \n",
    "\n",
    "\n",
    "    # Number of iterations\n",
    "    num_iterations = 3\n",
    "\n",
    "    # Initialize a list to store accuracy scores\n",
    "    ensemble_scores = []\n",
    "\n",
    "    # Perform multiple iterations of the VotingClassifier\n",
    "    for _ in range(num_iterations):\n",
    "        eclf1.fit(X_train_rf, y_train)\n",
    "        predictions = eclf1.predict(X_test_rf)\n",
    "        ensemble_acc = accuracy_score(y_test, predictions)\n",
    "        ensemble_scores.append(ensemble_acc)\n",
    "\n",
    "    print(\"Ensemble Voting Accuracy:\", ensemble_scores)\n",
    "    \n",
    "    # Calculate the mean accuracy of the ensemble\n",
    "    mean_ensemble_accuracy = np.mean(ensemble_scores)\n",
    "    print(\"Mean Ensemble Voting Accuracy:\", mean_ensemble_accuracy)\n",
    "\n",
    "    print(X_train_rf.columns)\n",
    "    accuracy_rate.append(mean_ensemble_accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(range(500,8500,500),accuracy_rate['ReliefF_accuracy'],color='blue', linestyle='solid', marker='o',\n",
    "         markerfacecolor='blue', markersize=10)\n",
    "\n",
    "title =plt.title('ReliefF Accuracy vs. No. of Genes', fontsize=20)\n",
    "plt.xlabel('No. of Genes', fontsize=16)\n",
    "plt.ylabel('ReliefF Accuracy', fontsize=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_plot = pd.DataFrame()\n",
    "accuracy_plot['ReliefF_accuracy'] = accuracy_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure the sizes of X_train and X_test.\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to obtain the mutual information values\n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "\n",
    "# to select the features\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "accuracy_rate = []\n",
    "clf1 = ExtraTreesClassifier()\n",
    "clf2 = LogisticRegression(penalty ='l2', C=0.1, solver='liblinear', random_state=4,max_iter = 1000)\n",
    "clf3 = SVC(gamma =1e-05, probability = True, decision_function_shape = 'ovo', kernel = 'linear')\n",
    "clf4 = AdaBoostClassifier()\n",
    "clf5= XGBClassifier()\n",
    "\n",
    "# Will take some time\n",
    "for i in range(500,8500,500):\n",
    "    \n",
    "    sel_ = SelectKBest(mutual_info_classif, k=i).fit(X_train, y_train)\n",
    "    # display features\n",
    "    best_feature=X_train.columns[sel_.get_support()]\n",
    "    X_train_m = sel_.transform(X_train)\n",
    "    X_test_m = sel_.transform(X_test)\n",
    "    X_train_m = pd.DataFrame(X_train_m,index= X_train_original.index, columns=best_feature)\n",
    "    X_test_m = pd.DataFrame(X_test_m,index= X_test_original.index, columns=best_feature)\n",
    "   \n",
    "    \n",
    "    eclf1 = VotingClassifier(estimators=[ ('logistic', clf2),('SVM', clf3),\n",
    "                                           ('XGB', clf5), \n",
    "                                          ('extratree',clf1), ('AdaBoost', clf4)\n",
    "                                        ], voting='hard')    \n",
    "    print(i)\n",
    "    \n",
    "    for clf in (clf1,clf2, clf3, clf4, clf5, eclf1):\n",
    "        clf.fit(X_train_m, y_train)\n",
    "        y_pred = clf.predict(X_test_m)\n",
    "        #df_test[clf.__class__.__name__] = y_pred\n",
    "        print(clf.__class__.__name__, round(accuracy_score(y_test, y_pred),3))\n",
    "    \n",
    "    # Number of iterations\n",
    "    num_iterations = 3\n",
    "\n",
    "    # Initialize a list to store accuracy scores\n",
    "    ensemble_scores = []\n",
    "\n",
    "    # Perform multiple iterations of the VotingClassifier\n",
    "    for _ in range(num_iterations):\n",
    "        eclf1.fit(X_train_m, y_train)\n",
    "        predictions = eclf1.predict(X_test_m)\n",
    "        ensemble_acc = accuracy_score(y_test, predictions)\n",
    "        ensemble_scores.append(ensemble_acc)\n",
    "\n",
    "    print(\"Ensemble Voting Accuracy:\", ensemble_scores)\n",
    "    \n",
    "    # Calculate the mean accuracy of the ensemble\n",
    "    mean_ensemble_accuracy = np.mean(ensemble_scores)\n",
    "    print(\"Mean Ensemble Voting Accuracy:\", mean_ensemble_accuracy)\n",
    "\n",
    "    print(X_train_m.columns)\n",
    "    accuracy_rate.append(mean_ensemble_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(range(500,8500,500),accuracy_rate,color='blue', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='red', markersize=10)\n",
    "plt.title('Mutual Info Accuracy vs. No. of Genes')\n",
    "plt.xlabel('No. of Genes')\n",
    "plt.ylabel('Mutual Info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccuracy_plot = pd.DataFrame()\n",
    "accuracy_plot['Mutual info'] = accuracy_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# anova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure the sizes of X_train and X_test.\n",
    "X_train.shape\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to determine the p-values with anova\n",
    "from sklearn.feature_selection import f_classif, f_regression\n",
    "# to select the features\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the top 10 features\n",
    "sel_ = SelectKBest(f_classif, k=1000).fit(X_train, y_train)\n",
    "\n",
    "# display selected feature names\n",
    "X_train.columns[sel_.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "accuracy_rate = []\n",
    "clf1 = ExtraTreesClassifier()\n",
    "clf2 = CatBoostClassifier(logging_level='Silent')\n",
    "clf3 = RandomForestClassifier(n_estimators=10, max_leaf_nodes=16, random_state=1,n_jobs=-1)\n",
    "clf4 = LogisticRegression(penalty ='l1', C=0.1, solver='liblinear', random_state=4,max_iter = 1000)\n",
    "clf5 = DecisionTreeClassifier()\n",
    "clf6 = GaussianNB()\n",
    "clf7 = SVC(gamma =1e-05, probability = True, decision_function_shape = 'ovo', kernel = 'linear')\n",
    "clf8 = KNeighborsClassifier()\n",
    "clf9 = AdaBoostClassifier()\n",
    "clf10= GradientBoostingClassifier() \n",
    "clf11= XGBClassifier()\n",
    "clf12 = BaggingClassifier(DecisionTreeClassifier(), n_estimators=10, max_samples=10, bootstrap=True, random_state=1)\n",
    "clf13 = MLPClassifier(max_iter = 1000)             \n",
    "#('LogisticRegression', clf4)\n",
    "\n",
    "# Will take some time\n",
    "for i in range(500,8500,500):\n",
    "    \n",
    "    sel_ = SelectKBest(f_classif, k=i).fit(X_train, y_train)\n",
    "    # display features\n",
    "    best_feature=X_train.columns[sel_.get_support()]\n",
    "    X_train_m = sel_.transform(X_train)\n",
    "    X_test_m = sel_.transform(X_test)\n",
    "    X_train_m = pd.DataFrame(X_train_m,index= X_train_original.index, columns=best_feature)\n",
    "    X_test_m = pd.DataFrame(X_test_m,index= X_test_original.index, columns=best_feature)\n",
    "   \n",
    "    \n",
    "    eclf1 = VotingClassifier(estimators=[('logistic', clf4),\n",
    "                                         ('SVM', clf7), #('RF',clf3), ('MLP',clf13)\n",
    "                                         #('Bagging',clf12), #('knieghbor',clf8),('decisiontree',clf5),\n",
    "                                           ('XGB', clf11),  #('GradientBoosting', clf10),\n",
    "                                           ('AdaBoost', clf9) ,('extratree',clf1) #('NaiveBayes', clf6) #,\n",
    "                                        ], voting='hard')     \n",
    "    print(i)\n",
    "    \n",
    "    for clf in (clf13,clf4, clf7,clf3, clf5,clf6, clf8, clf9, clf10, clf11, clf12,clf1, eclf1):\n",
    "        clf.fit(X_train_m, y_train)\n",
    "        y_pred = clf.predict(X_test_m)\n",
    "        print(clf.__class__.__name__, round(accuracy_score(y_test, y_pred),3))\n",
    "    \n",
    "    eclf1.fit(X_train_m, y_train)\n",
    "    predictions = eclf1.predict(X_test_m)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(X_train_m.columns)\n",
    "    accuracy_rate.append(round(accuracy_score(y_test, predictions),3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(range(500,8500,500),accuracy_rate,color='blue', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='red', markersize=10)\n",
    "plt.title('ANOVA Accuracy vs. No. of Genes')\n",
    "plt.xlabel('No. of Genes')\n",
    "plt.ylabel('ANOVA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "clf1 = ExtraTreesClassifier()\n",
    "clf2 = LogisticRegression(penalty ='l2', C=0.1, solver='liblinear', random_state=4,max_iter = 1000)\n",
    "clf3 = SVC(gamma =1e-05, probability = True, decision_function_shape = 'ovo', kernel = 'linear')\n",
    "clf4 = AdaBoostClassifier()\n",
    "clf5= XGBClassifier()            \n",
    "               \n",
    "\n",
    "\n",
    "df_test = pd.DataFrame()\n",
    "df_test['y_test'] = y_test\n",
    "for clf in (clf1,clf2, clf3, clf4, clf5):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    df_test[clf.__class__.__name__] = round(accuracy_score(y_test, y_pred),3)\n",
    "    print(clf.__class__.__name__, round(accuracy_score(y_test, y_pred),3))\n",
    "    #print(clf.__class__.__name__, round(roc_auc_score(y_test, y_pred),3))\n",
    "    #kfold = StratifiedKFold(n_splits=3)\n",
    "    #results = cross_val_score(clf, X_train, y_train, cv=kfold)\n",
    "    #print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf1 = ExtraTreesClassifier()\n",
    "clf2 = LogisticRegression(penalty ='l2', C=0.1, solver='liblinear', random_state=4,max_iter = 1000)\n",
    "clf3 = SVC(gamma =1e-05, probability = True, decision_function_shape = 'ovo', kernel = 'linear')\n",
    "clf4 = AdaBoostClassifier()\n",
    "clf5= XGBClassifier()            \n",
    "               \n",
    "eclf1 = VotingClassifier(estimators=[  ('logistic', clf2),('SVM', clf3),\n",
    "                                        ('XGB', clf5), ('extratree',clf1), \n",
    "                                        ('AdaBoost', clf4)\n",
    "                                        ], voting='hard') \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for clf in (clf1,clf2, clf3, clf4, clf5, eclf1):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, round(accuracy_score(y_test, y_pred),3))\n",
    "\n",
    "\n",
    "\n",
    "eclf1.fit(X_train, y_train)\n",
    "predictions = eclf1.predict(X_test)\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "confusion_mat = confusion_matrix(y_test, predictions)\n",
    "# Extract values from the confusion matrix\n",
    "true_positive = confusion_mat[1, 1]\n",
    "false_positive = confusion_mat[0, 1]\n",
    "true_negative = confusion_mat[0, 0]\n",
    "false_negative = confusion_mat[1, 0]\n",
    "\n",
    "# Calculate Sensitivity (True Positive Rate)\n",
    "sensitivity = true_positive / (true_positive + false_negative)\n",
    "\n",
    "# Calculate Specificity\n",
    "specificity = true_negative / (true_negative + false_positive)\n",
    "\n",
    "# Print the results\n",
    "print(f'Sensitivity (True Positive Rate): {sensitivity:.2f}')\n",
    "print(f'Specificity: {specificity:.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
