{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this code for compare diffrent types of feature selection techniques.\n",
    "experiment 2: compare feature selection methods following the initial use of the univariate ROC-AUC method, including\n",
    "two embedded techniques: LASSO and Random forest variable importance. Moreover, mutual information and ReliefF methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split, LeaveOneOut, KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, classification_report\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "import xgboost as xgb\n",
    "from feature_engine.selection import DropDuplicateFeatures, DropConstantFeatures\n",
    "from sklearn.feature_selection import SelectFromModel, VarianceThreshold\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN, BorderlineSMOTE, SVMSMOTE\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import roc_auc_score, plot_roc_curve\n",
    "from yellowbrick.classifier import ROCAUC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input the data (gene expression array) and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['riaz', 'gide', 'van allen', 'hugo', 'lee']\n",
    "name= filenames[1]\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['riaz', 'gide', 'van allen', 'hugo', 'lee']\n",
    "name= filenames[1]\n",
    "\n",
    "# Read logged data\n",
    "logged_data = pd.read_csv('data/' + name + '_logged_data.csv', index_col='Unnamed: 0').T\n",
    "print(logged_data.shape)\n",
    "print(logged_data.head())\n",
    "\n",
    "# Read labels\n",
    "label = pd.read_csv('data/' + name + '.Pre.Samples.corr.csv', index_col='Unnamed: 0')\n",
    "print(label.shape)\n",
    "print(label.head())\n",
    "\n",
    "# Count of labels\n",
    "print(label['Resp_NoResp'].value_counts())\n",
    "\n",
    "# Normalized count of labels\n",
    "print(label['Resp_NoResp'].value_counts(normalize=True))\n",
    "\n",
    "# Label encoding\n",
    "y = label.replace({'No_Response': 0, 'Response': 1})\n",
    "labels = ['No_Response', 'Response']  # for plotting convenience later on\n",
    "\n",
    "print(y.head())\n",
    "print(y['Resp_NoResp'].head())\n",
    "print(y.iloc[:, 0].head())\n",
    "print(y.iloc[:, 0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the immune related genes from immport\n",
    "IRG= pd.read_csv('IRG.csv')\n",
    "print(IRG.shape)\n",
    "IRG.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (Y)\n",
    "X = logged_data\n",
    "Y = y.iloc[:, 0]\n",
    "\n",
    "# Oversampling using BorderlineSMOTE\n",
    "ada = BorderlineSMOTE(\n",
    "    sampling_strategy='auto',  # samples only the minority class\n",
    "    random_state=0,  # for reproducibility\n",
    "    k_neighbors=5,\n",
    "    m_neighbors=10,\n",
    "    kind='borderline-1',\n",
    "    n_jobs=4\n",
    ")\n",
    "\n",
    "# Resample the data\n",
    "X_res, y_res = ada.fit_resample(X, Y)\n",
    "\n",
    "# Print the shapes of resampled data\n",
    "print(X_res.shape, y_res.shape)\n",
    "\n",
    "# Print the counts of the original and resampled target variable\n",
    "print(Y.value_counts(), y_res.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# separate dataset into train and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=0)\n",
    "\n",
    "# Display shapes of train and test sets\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "# Display first few rows of test set\n",
    "print(X_test.head())\n",
    "\n",
    "# Display normalized value counts of target variable in test set\n",
    "print(y_test.value_counts(normalize=True))\n",
    "\n",
    "# Keep a copy of the original datasets\n",
    "X_train_original = X_train.copy()\n",
    "X_test_original = X_test.copy()\n",
    "\n",
    "# Check shapes of train and test sets after copying\n",
    "print(X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use of the data selected by Univariate ROC-AUC Performance from experiment 1 to begin with experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('gide_X_train_uni_roc-auc.csv',index_col = 'Unnamed: 0')\n",
    "print(X_train.shape)\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('gide_X_test_uni_roc-auc.csv',index_col = 'Unnamed: 0')\n",
    "print(X_test.shape)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep acopy of the data selected by Univariate ROC-AUC Performance\n",
    "X_train_uni = X_train\n",
    "X_test_uni = X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# relieff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrebate import ReliefF\n",
    "fs = ReliefF()\n",
    "fs.fit(X_train.values, y_train.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "accuracy_rate = []\n",
    "clf1 = ExtraTreesClassifier()\n",
    "clf2 = LogisticRegression(penalty ='l2', C=0.1, solver='liblinear', random_state=4,max_iter = 1000)\n",
    "clf3 = SVC(gamma =1e-05, probability = True, decision_function_shape = 'ovo', kernel = 'linear')\n",
    "clf4 = AdaBoostClassifier()\n",
    "clf5= XGBClassifier()\n",
    "\n",
    "\n",
    "# Will take some time\n",
    "for i in range(5,300,5):\n",
    "    \n",
    "    indx_sort=fs.feature_importances_.argsort()[-i:][::-1]\n",
    "    X_train_rf= X_train.iloc[:,indx_sort]\n",
    "    X_test_rf= X_test.iloc[:,indx_sort]\n",
    "    \n",
    "    eclf1 = VotingClassifier(estimators=[ ('logistic', clf2),('SVM', clf3),\n",
    "                                           ('XGB', clf5), \n",
    "                                          ('extratree',clf1), ('AdaBoost', clf4)\n",
    "                                        ], voting='hard') \n",
    "    \n",
    "   \n",
    "    print(i)\n",
    "    \n",
    "    \n",
    "    for clf in (clf1,clf2, clf3, clf4, clf5, eclf1):\n",
    "        clf.fit(X_train_rf, y_train)\n",
    "        y_pred = clf.predict(X_test_rf)\n",
    "        print(clf.__class__.__name__, round(accuracy_score(y_test, y_pred),3))\n",
    "        \n",
    "  \n",
    "    # Number of iterations\n",
    "    num_iterations = 3\n",
    "\n",
    "    # Initialize a list to store accuracy scores\n",
    "    ensemble_scores = []\n",
    "\n",
    "    # Perform multiple iterations of the VotingClassifier\n",
    "    for _ in range(num_iterations):\n",
    "        eclf1.fit(X_train_rf, y_train)\n",
    "        predictions = eclf1.predict(X_test_rf)\n",
    "        ensemble_acc = accuracy_score(y_test, predictions)\n",
    "        ensemble_scores.append(ensemble_acc)\n",
    "\n",
    "    print(\"Ensemble Voting Accuracy:\", ensemble_scores)\n",
    "    \n",
    "    # Calculate the mean accuracy of the ensemble\n",
    "    mean_ensemble_accuracy = np.mean(ensemble_scores)\n",
    "    print(\"Mean Ensemble Voting Accuracy:\", mean_ensemble_accuracy)\n",
    "\n",
    "    print(X_train_rf.columns)\n",
    "    accuracy_rate.append(mean_ensemble_accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(range(5,300,5),accuracy_rate,color='blue', linestyle='solid', marker='o',\n",
    "         markerfacecolor='blue', markersize=10)\n",
    "\n",
    "title =plt.title('ReliefF Accuracy vs. No. of Genes', fontsize=20)\n",
    "plt.xlabel('No. of Genes', fontsize=16)\n",
    "plt.ylabel('ReliefF Accuracy', fontsize=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select top 95 features ranked by Relieff, as obtainted in the increment feature selection above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx_sort=fs.feature_importances_.argsort()[-95:][::-1]\n",
    "indx_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rf= X_train.iloc[:,indx_sort]\n",
    "X_train_rf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_rf= X_test.iloc[:,indx_sort]\n",
    "X_test_rf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRG_feat=X_train_rf.columns.intersection(IRG['Symbol'])\n",
    "IRG_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_rf\n",
    "X_test= X_test_rf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save a copy of reliefF result for the following experiment \n",
    "X_train.to_csv('gide_X_train_uni_reliefF.csv')\n",
    "X_test.to_csv('gide_X_test_uni_reliefF.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use of the data selected by Univariate ROC-AUC Performance\n",
    "X_train = X_train_uni\n",
    "X_test = X_test_uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure the sizes of X_train and X_test.\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to obtain the mutual information values\n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "\n",
    "# to select the features\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "accuracy_rate = []\n",
    "clf1 = ExtraTreesClassifier()\n",
    "clf2 = LogisticRegression(penalty ='l2', C=0.1, solver='liblinear', random_state=4,max_iter = 1000)\n",
    "clf3 = SVC(gamma =1e-05, probability = True, decision_function_shape = 'ovo', kernel = 'linear')\n",
    "clf4 = AdaBoostClassifier()\n",
    "clf5= XGBClassifier()\n",
    "\n",
    "# Will take some time\n",
    "for i in range(5,300,5):\n",
    "    \n",
    "    sel_ = SelectKBest(mutual_info_classif, k=i).fit(X_train, y_train)\n",
    "    # display features\n",
    "    best_feature=X_train.columns[sel_.get_support()]\n",
    "    X_train_m = sel_.transform(X_train)\n",
    "    X_test_m = sel_.transform(X_test)\n",
    "    X_train_m = pd.DataFrame(X_train_m,index= X_train_original.index, columns=best_feature)\n",
    "    X_test_m = pd.DataFrame(X_test_m,index= X_test_original.index, columns=best_feature)\n",
    "   \n",
    "    \n",
    "    eclf1 = VotingClassifier(estimators=[ ('logistic', clf2),('SVM', clf3),\n",
    "                                           ('XGB', clf5), \n",
    "                                          ('extratree',clf1), ('AdaBoost', clf4)\n",
    "                                        ], voting='hard')    \n",
    "    print(i)\n",
    "    \n",
    "    for clf in (clf1,clf2, clf3, clf4, clf5, eclf1):\n",
    "        clf.fit(X_train_m, y_train)\n",
    "        y_pred = clf.predict(X_test_m)\n",
    "        #df_test[clf.__class__.__name__] = y_pred\n",
    "        print(clf.__class__.__name__, round(accuracy_score(y_test, y_pred),3))\n",
    "    \n",
    "    # Number of iterations\n",
    "    num_iterations = 3\n",
    "\n",
    "    # Initialize a list to store accuracy scores\n",
    "    ensemble_scores = []\n",
    "\n",
    "    # Perform multiple iterations of the VotingClassifier\n",
    "    for _ in range(num_iterations):\n",
    "        eclf1.fit(X_train_m, y_train)\n",
    "        predictions = eclf1.predict(X_test_m)\n",
    "        ensemble_acc = accuracy_score(y_test, predictions)\n",
    "        ensemble_scores.append(ensemble_acc)\n",
    "\n",
    "    print(\"Ensemble Voting Accuracy:\", ensemble_scores)\n",
    "    \n",
    "    # Calculate the mean accuracy of the ensemble\n",
    "    mean_ensemble_accuracy = np.mean(ensemble_scores)\n",
    "    print(\"Mean Ensemble Voting Accuracy:\", mean_ensemble_accuracy)\n",
    "\n",
    "    print(X_train_m.columns)\n",
    "    accuracy_rate.append(mean_ensemble_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(range(5,300,5),accuracy_rate,color='blue', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='red', markersize=10)\n",
    "plt.title('Mutual Info Accuracy vs. No. of Genes')\n",
    "plt.xlabel('No. of Genes')\n",
    "plt.ylabel('Mutual Info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use of the data selected by Univariate ROC-AUC Performance\n",
    "X_train = X_train_uni\n",
    "X_test = X_test_uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure the sizes of X_train and X_test.\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_uni.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sel_ = SelectFromModel(\n",
    "    LogisticRegression(C=0.9, penalty='l1', solver='liblinear', random_state=0))\n",
    "\n",
    "sel_.fit((X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of features which coefficient was shrank to zero:\n",
    "np.sum(sel_.estimator_.coef_ == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can then remove the features from the training and testing set\n",
    "# like this:\n",
    "\n",
    "X_train_lasso = sel_.transform(X_train)\n",
    "X_test_lasso = sel_.transform(X_test)\n",
    "\n",
    "X_train_lasso.shape, X_test_lasso.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_names_lasso = X_train.columns[sel_.get_support()]\n",
    "feat_names_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train_lasso = pd.DataFrame(X_train_lasso,index= X_train_uni.index, columns=feat_names_lasso)\n",
    "X_train_lasso.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_test_lasso = pd.DataFrame(X_test_lasso,index= X_test_uni.index, columns=feat_names_lasso)\n",
    "X_test_lasso.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lasso performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf1 = ExtraTreesClassifier()\n",
    "clf2 = LogisticRegression(penalty ='l2', C=0.1, solver='liblinear', random_state=4,max_iter = 1000)\n",
    "clf3 = SVC(gamma =1e-05, probability = True, decision_function_shape = 'ovo', kernel = 'linear')\n",
    "clf4 = AdaBoostClassifier()\n",
    "clf5= XGBClassifier()            \n",
    "               \n",
    "eclf1 = VotingClassifier(estimators=[  ('logistic', clf2),('SVM', clf3),\n",
    "                                        ('XGB', clf5), ('extratree',clf1), \n",
    "                                        ('AdaBoost', clf4)\n",
    "                                        ], voting='hard') \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for clf in (clf1,clf2, clf3, clf4, clf5, eclf1):\n",
    "    clf.fit(X_train_lasso, y_train)\n",
    "    y_pred = clf.predict(X_test_lasso)\n",
    "    print(clf.__class__.__name__, round(accuracy_score(y_test, y_pred),3))\n",
    "\n",
    "\n",
    "\n",
    "eclf1.fit(X_train_lasso, y_train)\n",
    "predictions = eclf1.predict(X_test_lasso)\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "confusion_mat = confusion_matrix(y_test, predictions)\n",
    "# Extract values from the confusion matrix\n",
    "true_positive = confusion_mat[1, 1]\n",
    "false_positive = confusion_mat[0, 1]\n",
    "true_negative = confusion_mat[0, 0]\n",
    "false_negative = confusion_mat[1, 0]\n",
    "\n",
    "# Calculate Sensitivity (True Positive Rate)\n",
    "sensitivity = true_positive / (true_positive + false_negative)\n",
    "\n",
    "# Calculate Specificity\n",
    "specificity = true_negative / (true_negative + false_positive)\n",
    "\n",
    "# Print the results\n",
    "print(f'Sensitivity (True Positive Rate): {sensitivity:.2f}')\n",
    "print(f'Specificity: {specificity:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Features by Random Forests Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use of the data selected by Univariate ROC-AUC Performance\n",
    "X_train = X_train_uni\n",
    "X_test = X_test_uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure the sizes of X_train and X_test.\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_uni.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features using the impotance derived from\n",
    "# random forests\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "sel_ = SelectFromModel(RandomForestClassifier(n_estimators=10, random_state=42))\n",
    "sel_.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove features with zero coefficient from dataset and parse again as dataframe (output of sklearn is numpy array)\n",
    "\n",
    "X_train_rf = pd.DataFrame(sel_.transform(X_train),index= X_train.index, columns= X_train.columns[(sel_.get_support())])\n",
    "X_test_rf = pd.DataFrame(sel_.transform(X_test),index= X_test.index, columns= X_train.columns[(sel_.get_support())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rf.shape, X_test_rf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_rf\n",
    "X_test= X_test_rf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF importance performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf1 = ExtraTreesClassifier()\n",
    "clf2 = LogisticRegression(penalty ='l2', C=0.1, solver='liblinear', random_state=4,max_iter = 1000)\n",
    "clf3 = SVC(gamma =1e-05, probability = True, decision_function_shape = 'ovo', kernel = 'linear')\n",
    "clf4 = AdaBoostClassifier()\n",
    "clf5= XGBClassifier()            \n",
    "               \n",
    "eclf1 = VotingClassifier(estimators=[  ('logistic', clf2),('SVM', clf3),\n",
    "                                        ('XGB', clf5), ('extratree',clf1), \n",
    "                                        ('AdaBoost', clf4)\n",
    "                                        ], voting='hard') \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for clf in (clf1,clf2, clf3, clf4, clf5, eclf1):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, round(accuracy_score(y_test, y_pred),3))\n",
    "\n",
    "\n",
    "\n",
    "eclf1.fit(X_train, y_train)\n",
    "predictions = eclf1.predict(X_test)\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "confusion_mat = confusion_matrix(y_test, predictions)\n",
    "# Extract values from the confusion matrix\n",
    "true_positive = confusion_mat[1, 1]\n",
    "false_positive = confusion_mat[0, 1]\n",
    "true_negative = confusion_mat[0, 0]\n",
    "false_negative = confusion_mat[1, 0]\n",
    "\n",
    "# Calculate Sensitivity (True Positive Rate)\n",
    "sensitivity = true_positive / (true_positive + false_negative)\n",
    "\n",
    "# Calculate Specificity\n",
    "specificity = true_negative / (true_negative + false_positive)\n",
    "\n",
    "# Print the results\n",
    "print(f'Sensitivity (True Positive Rate): {sensitivity:.2f}')\n",
    "print(f'Specificity: {specificity:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
